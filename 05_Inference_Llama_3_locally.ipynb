{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf7b96-6076-49f3-ba2e-37dc23a78fe7",
   "metadata": {},
   "source": [
    "# Inference Demo with Llama3 Locally\n",
    "\n",
    "Once the models you want have been downloaded, you can run the model locally using the command below:\n",
    "\n",
    "Different models require different model-parallel (MP) values:\n",
    "\n",
    "|  Model | MP |\n",
    "|--------|----|\n",
    "| 8B     | 1  |\n",
    "| 70B    | 8  |\n",
    "\n",
    "## Llama3 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce85d8a5-4cf0-485d-828b-79cb28d7c348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>\n",
       "   Step 1: Open a terminal session \n",
       "   Step 2: Navigate to the directory by executing: : <font color=\"green\">cd /data/ai/tutorial/Llama3_on_HPG/llama3</font>\n",
       "   Step 3: Request compute resources by executing: <font color=\"green\">srun -p hpg-ai --gpus=1 --ntasks=1 --cpus-per-task=4 --mem 50gb  --pty -u bash -i</font>\n",
       "   Step 4: Load the Llama3 module by executing: <font color=\"green\">ml llama/3</font>\n",
       "   Step 5: Run the Llama3 8b pretraining on 1 GPUs, example 1: <font color=\"green\">\n",
       "           torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
       "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-Instruct/ \\\n",
       "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-Instruct/tokenizer.model \\\n",
       "               --max_seq_len 512 --max_batch_size 6</font>\n",
       "   Step 6: Run the Llama3 8b pretraining on 1 GPUs, example 2: <font color=\"green\">\n",
       "           torchrun --nproc_per_node 1 example_text_completion.py \\\n",
       "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B/ \\\n",
       "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B/tokenizer.model \\\n",
       "               --max_seq_len 512 --max_batch_size 6</font>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session \n",
    "   Step 2: Navigate to the directory by executing: : <font color=\"green\">cd /data/ai/tutorial/Llama3_on_HPG/llama3</font>\n",
    "   Step 3: Request compute resources by executing: <font color=\"green\">srun -p hpg-ai --gpus=1 --ntasks=1 --cpus-per-task=4 --mem 50gb  --pty -u bash -i</font>\n",
    "   Step 4: Load the Llama3 module by executing: <font color=\"green\">ml llama/3</font>\n",
    "   Step 5: Run the Llama3 8b pretraining on 1 GPUs, example 1: <font color=\"green\">\n",
    "           torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-Instruct/ \\\n",
    "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-Instruct/tokenizer.model \\\n",
    "               --max_seq_len 512 --max_batch_size 6</font>\n",
    "   Step 6: Run the Llama3 8b pretraining on 1 GPUs, example 2: <font color=\"green\">\n",
    "           torchrun --nproc_per_node 1 example_text_completion.py \\\n",
    "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B/ \\\n",
    "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B/tokenizer.model \\\n",
    "               --max_seq_len 512 --max_batch_size 6</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41b2b4-9ae8-42c1-b2b0-18e7a3b8c6df",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- The `â€“nproc_per_node` should be set to the `MP` value for the model you are using.\n",
    "- Adjust the `max_seq_len` and `max_batch_size` parameters as needed.\n",
    "- This example runs the [example_chat_completion.py](./llama3/example_chat_completion.py) found in this repository, but you can change that to a different .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf495e6c-14d5-4d48-808a-51dfa238d4cd",
   "metadata": {},
   "source": [
    "## Llama3 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d37481-bd39-4096-8833-982c020f92d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>\n",
       "   Step 1: Open a terminal session \n",
       "   Step 2: Navigate to the directory by executing: : <font color=\"green\">cd /data/ai/tutorial/Llama3_on_HPG/llama3</font>\n",
       "   Step 3: Request compute resources by executing: <font color=\"green\">srun -p hpg-ai --gpus=8 --ntasks=8 --cpus-per-task=4 --mem 50gb  --pty -u bash -i</font>\n",
       "   Step 4: Load the Llama3 module by executing: <font color=\"green\">ml llama/3</font>\n",
       "   Step 5: Run the Llama3 70b pretraining on 8 GPUs, example 1: <font color=\"green\">\n",
       "           torchrun --nproc_per_node 8 example_chat_completion.py \\\n",
       "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B-Instruct/ \\\n",
       "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B-Instruct/tokenizer.model \\\n",
       "               --max_seq_len 512 --max_batch_size 6</font>\n",
       "  Step 6: Run the Llama3 70b pretraining on 8 GPUs, example 2: <font color=\"green\">\n",
       "           torchrun --nproc_per_node 8 example_text_completion.py \\\n",
       "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B/ \\\n",
       "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B/tokenizer.model \\\n",
       "               --max_seq_len 512 --max_batch_size 6</font>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session \n",
    "   Step 2: Navigate to the directory by executing: : <font color=\"green\">cd /data/ai/tutorial/Llama3_on_HPG/llama3</font>\n",
    "   Step 3: Request compute resources by executing: <font color=\"green\">srun -p hpg-ai --gpus=8 --ntasks=8 --cpus-per-task=4 --mem 50gb  --pty -u bash -i</font>\n",
    "   Step 4: Load the Llama3 module by executing: <font color=\"green\">ml llama/3</font>\n",
    "   Step 5: Run the Llama3 70b pretraining on 8 GPUs, example 1: <font color=\"green\">\n",
    "           torchrun --nproc_per_node 8 example_chat_completion.py \\\n",
    "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B-Instruct/ \\\n",
    "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B-Instruct/tokenizer.model \\\n",
    "               --max_seq_len 512 --max_batch_size 6</font>\n",
    "  Step 6: Run the Llama3 70b pretraining on 8 GPUs, example 2: <font color=\"green\">\n",
    "           torchrun --nproc_per_node 8 example_text_completion.py \\\n",
    "               --ckpt_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B/ \\\n",
    "               --tokenizer_path /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-70B/tokenizer.model \\\n",
    "               --max_seq_len 512 --max_batch_size 6</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4b845-0772-4e21-b998-a26afcd9ca47",
   "metadata": {},
   "source": [
    "If you encounter `RuntimeError: CUDA error: invalid device ordinal`, here is something you can do.\n",
    "\n",
    "* Check Available GPUs: Verify the number of available GPUs on your system to ensure the local rank is within the valid range. Run the following in your terminal\n",
    "\n",
    "   ```shell\n",
    "   python\n",
    "   import torch\n",
    "   print(torch.cuda.device_count())\n",
    "   ```\n",
    "   \n",
    "If you cannot get 8 GPUs available from `srun`, try allocating the resources from a Jupyter notebook on Open OnDemand (OOD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd8e0ec-b767-4c23-a97c-0409a19df2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0bf3b-a8b7-4b80-94a5-fcefd49a2f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
