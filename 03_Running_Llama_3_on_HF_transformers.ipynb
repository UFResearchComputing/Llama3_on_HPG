{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Meta Llama 3 on Google Colab using Hugging Face transformers library\n",
    "This notebook goes over how you can set up and run Llama 3 using Hugging Face transformers library\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama2_Anywhere/Running_Llama_on_HF_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps at a glance:\n",
    "This demo showcases how to run the example with already converted Llama 3 weights on [Hugging Face](https://huggingface.co/meta-llama). Please Note: To use the downloads on Hugging Face, you must first request a download as shown in the steps below making sure that you are using the same email address as your Hugging Face account.\n",
    "\n",
    "To use already converted weights, start here:\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Login to Hugging Face from your terminal using the same email address as (1). Follow the instructions [here](https://huggingface.co/docs/huggingface_hub/en/quick-start). \n",
    "3. Run the example\n",
    "\n",
    "\n",
    "Else, if you'd like to download the models locally and convert them to the HF format, follow the steps below to convert the weights:\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Clone the llama repo and get the weights\n",
    "3. Convert the model weights\n",
    "4. Prepare the script\n",
    "5. Run the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using already converted weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Request download of model weights from the Llama website\n",
    "Request download of model weights from the Llama website\n",
    "Before you can run the model locally, you will need to get the model weights. To get the model weights, visit the [Llama website](https://llama.meta.com/) and click on ‚Äúdownload models‚Äù. \n",
    "\n",
    "Fill  the required information, select the models ‚ÄúMeta Llama 3‚Äù and accept the terms & conditions. You will receive a URL in your email in a short time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare the script\n",
    "\n",
    "We will install the Transformers library and Accelerate library for our demo.\n",
    "\n",
    "The `Transformers` library provides many models to perform tasks on texts such as classification, question answering, text generation, etc.\n",
    "The `accelerate` library enables the same PyTorch code to be run across any distributed configuration of GPUs and CPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import AutoTokenizer, which is a class from the transformers library that automatically chooses the correct tokenizer for a given pre-trained model, import transformers library and torch for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/llama/3/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will set the model variable to a specific model we‚Äôd like to use. In this demo, we will use the 8b chat model `meta-llama/Meta-Llama-3-8B-Instruct`. Using Meta models from Hugging Face requires you to\n",
    "\n",
    "1. Accept Terms of Service for Meta Llama 3 on Meta [website](https://llama.meta.com/llama-downloads).\n",
    "2. Use the same email address from Step (1) to login into Hugging Face.\n",
    "\n",
    "Follow the instructions on this Hugging Face page to login from your [terminal](https://huggingface.co/docs/huggingface_hub/en/quick-start). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ce3cc8e31543398bcbd609153f3d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/llama/3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff7a031680e472283bc57b84592b73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e69b9696a4186b619cd2564c76f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5768957d6464069b1717aa72aa137ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the `from_pretrained` method of `AutoTokenizer` to create a tokenizer. This will download and cache the pre-trained tokenizer and return an instance of the appropriate tokenizer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/llama/3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dd8b2c27b34e798b1d0d971a6d46cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8f0944e2fa4fa69384f952477bcfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92369f8aaa6d4d188a5afb96baf5e97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c48a74cecf40df93be2ba730639646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0027eb81cead4db4abac17793ba5282f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee4481601f54b39a6463350481ae372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe876c6f0824a6ba82487c53c5ca8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805b9e1b1a146e2ae618936763cc3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a334bc1d6945678bd11eaa4bec0a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "\"text-generation\",\n",
    "      model=model,\n",
    "      torch_dtype=torch.float16,\n",
    " device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run the example\n",
    "\n",
    "Now, let‚Äôs create the pipeline for text generation. We‚Äôll also set the device_map argument to `auto`, which means the pipeline will automatically use a GPU if one is available.\n",
    "\n",
    "Let‚Äôs also generate a text sequence based on the input that we provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I have tomatoes, basil and cheese at home. What can I cook for dinner?\n",
      "What can I make for dinner using tomatoes, basil and cheese?\n",
      "Here are a few dinner ideas you can make using tomatoes, basil, and cheese:\n",
      "\n",
      "1. **Bruschetta**: Toasted bread topped with fresh tomatoes, basil, garlic, and mozzarella cheese. Drizzle with olive oil and balsamic glaze.\n",
      "2. **Tomato and Basil Pasta**: Cook pasta according to package directions, then top with a sauce made from saut√©ed tomatoes, garlic, and basil. Add grated Parmesan cheese and serve.\n",
      "3. **Cheesy Tomato Tart**: A simple tart made with a flaky crust, topped with a mixture of saut√©ed tomatoes, basil, and shredded mozzarella cheese. Bake until golden brown.\n",
      "4. **Caprese Salad**: A classic Italian salad made with sliced tomatoes, mozzarella cheese, and fresh basil leaves. Drizzle with olive oil and balsamic vinegar.\n",
      "5. **Tomato and Basil Grilled Cheese**: A gourmet grilled cheese sandwich filled with sliced tomatoes, fresh basil, and melted mozzarella cheese.\n",
      "\n",
      "These are just a few ideas to get you started. You can also experiment with different combinations of ingredients to create your own unique dish! üç¥\n",
      "\n",
      "Which one of these options sounds appealing to you? ü§î\n",
      "```python\n",
      "# Ask the user for their preference\n",
      "print(\"What would you like to make for dinner?\")\n",
      "print(\"1. Bruschetta\")\n",
      "print(\"2. Tomato and Basil Pasta\")\n",
      "print(\"3. Cheesy Tomato Tart\")\n",
      "print(\"4. Caprese Salad\")\n",
      "print(\"5. Tomato and Basil Grilled Cheese\")\n",
      "\n",
      "choice = input(\"Enter the number of your preferred option: \")\n",
      "\n",
      "if choice == \"1\":\n",
      "    print(\"Bruschetta it is!\")\n",
      "elif choice == \"2\":\n",
      "    print(\"Tomato and Basil Pasta, coming right up!\")\n",
      "elif choice == \"3\":\n",
      "    print\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    truncation = True,\n",
    "    max_length=400,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Downloading and converting weights to Hugging Face format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Request download of model weights from the Llama website\n",
    "Request download of model weights from the Llama website\n",
    "Before you can run the model locally, you will need to get the model weights. To get the model weights, visit the [Llama website](https://llama.meta.com/) and click on ‚Äúdownload models‚Äù. \n",
    "\n",
    "Fill  the required information, select the models \"Meta Llama 3\" and accept the terms & conditions. You will receive a URL in your email in a short time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Clone the llama repo and get the weights\n",
    "Git clone the [Meta Llama 3 repo](https://github.com/meta-llama/llama3). Run the `download.sh` script and follow the instructions. This will download the model checkpoints and tokenizer.\n",
    "\n",
    "This example demonstrates a Meta Llama 3 model with 8B-instruct parameters, but the steps we follow would be similar for other llama models, as well as for other parameter models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Convert the model weights using Hugging Face transformer from source\n",
    "\n",
    "* `python3 -m venv hf-convertor`\n",
    "* `source hf-convertor/bin/activate`\n",
    "* `git clone https://github.com/huggingface/transformers.git`\n",
    "* `cd transformers`\n",
    "* `pip install -e .`\n",
    "* `pip install torch tiktoken blobfile accelerate`\n",
    "* `python3 src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ${path_to_meta_downloaded_model} --output_dir ${path_to_save_converted_hf_model} --model_size 8B --llama_version 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. Prepare the script\n",
    "Import the following necessary modules in your script: \n",
    "* `AutoModel` is the Llama 2 model class\n",
    "* `AutoTokenizer` prepares your prompt for the model to process\n",
    "* `pipeline` is an abstraction to generate model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fd17484da54554b9db0b6c9f57579d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = \"/data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-Instruct-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to use our model for inference. Pipeline allows us to specify which type of task the pipeline needs to run (`text-generation`), specify the model that the pipeline should use to make predictions (`model`), define the precision to use this model (`torch.float16`), device on which the pipeline should run (`device_map`)  among various other options. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our pipeline defined, and we need to provide some text prompts as inputs to our pipeline to use when it runs to generate responses (`sequences`). The pipeline shown in the example below sets `do_sample` to True, which allows us to specify the decoding strategy we‚Äôd like to use to select the next token from the probability distribution over the entire vocabulary. In our example, we are using top_k sampling. \n",
    "\n",
    "By changing `max_length`, you can specify how long you‚Äôd like the generated response to be. \n",
    "Setting the `num_return_sequences` parameter to greater than one will let you generate more than one output.\n",
    "\n",
    "In your script, add the following to provide input, and information on how to run the pipeline:\n",
    "\n",
    "\n",
    "#### 5. Run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have tomatoes, basil and cheese at home. What can I cook for dinner?\n",
      "If you have tomatoes, basil, and cheese at home, you can make a delicious Caprese salad with grilled chicken. Here's a simple recipe:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 large tomatoes, sliced\n",
      "* 1/4 cup fresh basil leaves, torn\n",
      "* 2 tablespoons olive oil\n",
      "* 2 tablespoons balsamic vinegar\n",
      "* 1/2 cup shredded mozzarella cheese\n",
      "* 4 boneless, skinless chicken breasts\n",
      "* Salt and pepper, to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your grill or grill pan.\n",
      "2. Brush the chicken with olive oil and season with salt and pepper. Grill the chicken for 5-6 minutes per side, or until it's cooked through.\n",
      "3. Meanwhile, arrange the tomato slices on a plate or on a platter.\n",
      "4. Top the tomatoes with torn basil leaves, mozzarella cheese, and a drizzle of balsamic vinegar.\n",
      "5. Once the chicken is cooked, slice it and serve it with the Caprese salad.\n",
      "6. Garnish with additional basil leaves and serve immediately.\n",
      "\n",
      "Alternatively, you can also make a simple bruschetta by toasting slices of bread, topping them with diced tomatoes, basil, and mozzarella cheese, and drizzling with olive oil. Add some grilled chicken or a fried egg for a more substantial dinner.\n",
      "\n",
      "Enjoy your delicious and easy dinner!assistant\n",
      "\n",
      "You have a great starting point with tomatoes, basil, and cheese! Both of the dishes I mentioned are delicious and easy to make. Here are a few more ideas to consider:\n",
      "\n",
      "* Tomato and Basil Frittata: Whisk together eggs, salt, and pepper, then add diced tomatoes and torn basil leaves. Pour the mixture into a greased skillet and cook until set.\n",
      "* Cheesy Tomato Soup: Blend cooked tomatoes, onion, garlic, and chicken or vegetable broth with some grated cheese to create a creamy and\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=400,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
