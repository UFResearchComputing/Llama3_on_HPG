{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ec8829-f4f5-422b-b2de-f6adde6e98ca",
   "metadata": {},
   "source": [
    "# Deploy Llama3 with TensorRT-LLM\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook, we will walk through on converting Mistral into the TensorRT format. TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.\n",
    "\n",
    "Once the TensorRT engine is build, you can use the run.py script provided at the end of this notebook or use this engine as in input to the Triton Inference Server.\n",
    "\n",
    "See the [Github repo](https://github.com/NVIDIA/TensorRT-LLM) for more examples and documentation!\n",
    "\n",
    "Deployment powered by [Brev.dev](https://twitter.com/brevdev) and the link for the [notebook](https://console.brev.dev/notebook/llama3-tensorrtllm-deployment).\n",
    "\n",
    "## Step 1 - Install TensorRT-LLM\n",
    "\n",
    "We first install TensorRT-LLM, which is already installed in the `nlp-1.3` and `Llama3` Jupyter kernels. You can choose either of these to run the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc104-c6a1-4195-a7c3-7f7628c913b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2bb1d-81b2-4f33-8bae-e87388524265",
   "metadata": {},
   "source": [
    "## Step 2 - Download Llama3 model weights\n",
    "\n",
    "Llama3 is a gated model which means you'll need to request approval on their respository and generate a HF token. This usually takes about 20 minutes! The good news is that we have already downloaded the Llama3 model, which is located at `/data/ai/models/nlp/llama/models_llama3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa79d9-b100-4bbd-aee9-72e8cf636c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148af093-09c4-4623-a506-6da8e4f00e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_hub.login(\"<ENTER TOKEN HERE>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2467f-2d91-4ee0-99f9-cefe88036dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_hub.snapshot_download(\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"llama3-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c3c6f-b5de-4a6e-a485-d294bbeb7edf",
   "metadata": {},
   "source": [
    "## Step 3 - Convert checkpoints into safetensors and build the TRT engine\n",
    "\n",
    "There are 2 substeps here. The first is converting the raw huggingface model into safetensors which is a safe and fast format for storing tensors.\n",
    "\n",
    "Next we build the TensorRT engine. This is where the magic happens. We take the converted safetensors model and convert it into a `TensorRT engine`. Engines are optimized versions of models built to run lightening fast on the current machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1cb7f-f8a9-4113-bb8d-fa7388c64681",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -L https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/llama/convert_checkpoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4b9d7-da6e-4d19-b5bb-c8f0cd49e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert_checkpoint.py --model_dir /data/ai/models/nlp/llama/models_llama3/Meta-Llama-3-8B-hf \\\n",
    "    --output_dir ./llama3-safetensors \\\n",
    "    --dtype bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a0b83-3823-4e99-854c-0812b7bbbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtllm-build --checkpoint_dir llama3-safetensors \\\n",
    "    --output_dir ./llama3engine_bf16_1gpu \\\n",
    "    --gpt_attention_plugin bfloat16 \\\n",
    "    --gemm_plugin bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef031d-e286-434c-851d-e2bf82ce772a",
   "metadata": {},
   "source": [
    "## Step 4 - Run the model using the example script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47b334-6b56-419a-858e-3fe60e5bead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de47af-35e4-49ee-8f3b-93134b6fb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./TensorRT-LLM/examples/run.py --engine_dir=llama3engine_bf16_1gpu \\\n",
    "    --max_output_len 100 \\\n",
    "    --tokenizer_dir llama3-hf \\\n",
    "    --input_text \"How do I count to nine in French?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
